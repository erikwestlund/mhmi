
## MAR: Listwise Deletion, Naive Model

```{r}
#| label: mar-naive
#| echo: true

mar_data <- simulate_birth_data(n = 1000, mcar_rate = 0.5, mar_logit_shift = 0.5)

mar_cc <- mar_data |> filter(!is.na(X_mar))

# Naive: ignore education
fit_mar_naive <- lm(Y ~ X_mar, data = mar_cc)
summary(fit_mar_naive)
```

## MAR: Adjusting for Education

```{r}
#| label: mar-adjust
#| echo: true

fit_mar_adj <- lm(Y ~ X_mar + E, data = mar_cc)
summary(fit_mar_adj)
```


## MAR: Complete-Case Limitations

- Naive CC (Y ~ X only) is biased because we changed the education mix.
- If we condition on E, the slope for X improves — but we still threw away a lot of mothers and lost power.

```{r}
#| label: sim-impute-helpers
#| include: false

analyze_mar_strategies_once <- function(n = 1000, mcar_rate = 0.6) {
  dat <- simulate_birth_data(n = n, mcar_rate = mcar_rate)
  full_fit <- lm(Y ~ X + E, data = dat)

  beta_true <- coef(full_fit)[["X"]]

  cc <- dat |> filter(!is.na(X_mar))

  fit_cc_naive <- lm(Y ~ X_mar, data = cc)
  fit_cc_adj   <- lm(Y ~ X_mar + E, data = cc)

  mean_x <- mean(cc$X_mar, na.rm = TRUE)
  dat_mean <- dat |>
    mutate(
      X_imp_mean = if_else(is.na(X_mar), mean_x, X_mar)
    )
  fit_mean <- lm(Y ~ X_imp_mean + E, data = dat_mean)

  dat_mean_ind <- dat |>
    mutate(
      X_miss_ind = as.integer(is.na(X_mar)),
      X_imp_mean = if_else(is.na(X_mar), mean(cc$X_mar, na.rm = TRUE), X_mar)
    )
  fit_mean_ind <- lm(Y ~ X_imp_mean + X_miss_ind + E, data = dat_mean_ind)

  reg_x <- lm(X ~ E, data = dat, subset = !is.na(X_mar))
  dat_reg <- dat |>
    mutate(
      X_imp_reg = if_else(
        is.na(X_mar),
        predict(reg_x, newdata = dat),
        X_mar
      )
    )
  fit_reg <- lm(Y ~ X_imp_reg + E, data = dat_reg)

  summary_tbl <- tibble(
    method = c(
      "Full data (Y ~ X + E)",
      "MAR CC: naive (Y ~ X)",
      "MAR CC: adjust E",
      "Mean imputation",
      "Mean + missing indicator",
      "Regression imputation (E only)"
    ),
    estimate = c(
      coef(full_fit)[["X"]],
      coef(fit_cc_naive)[["X_mar"]],
      coef(fit_cc_adj)[["X_mar"]],
      coef(fit_mean)[["X_imp_mean"]],
      coef(fit_mean_ind)[["X_imp_mean"]],
      coef(fit_reg)[["X_imp_reg"]]
    ),
    se = c(
      coef(summary(full_fit))[["X", "Std. Error"]],
      coef(summary(fit_cc_naive))[["X_mar", "Std. Error"]],
      coef(summary(fit_cc_adj))[["X_mar", "Std. Error"]],
      coef(summary(fit_mean))[["X_imp_mean", "Std. Error"]],
      coef(summary(fit_mean_ind))[["X_imp_mean", "Std. Error"]],
      coef(summary(fit_reg))[["X_imp_reg", "Std. Error"]]
    ),
    beta_true = beta_true
  )

  list(
    dat_full   = dat,
    cc         = cc,
    dat_mean   = dat_mean,
    dat_mean_ind = dat_mean_ind,
    dat_reg    = dat_reg,
    full_fit   = full_fit,
    fit_cc_naive = fit_cc_naive,
    fit_cc_adj   = fit_cc_adj,
    fit_mean     = fit_mean,
    fit_mean_ind = fit_mean_ind,
    fit_reg      = fit_reg,
    summary      = summary_tbl
  )
}
```

```{r}
#| label: mar-strategies-setup
#| echo: false

mar_obj <- analyze_mar_strategies_once(n = 1000, mcar_rate = 0.6)
mar_strats <- mar_obj$summary
```

```{r}
#| label: partial-plot-helpers
#| include: false

make_partial_df <- function(dat, x_var) {
  # dat must contain Y, E, and the x_var column
  x <- dat[[x_var]]
  keep <- !is.na(x) & !is.na(dat$Y) & !is.na(dat$E)
  x <- x[keep]
  y <- dat$Y[keep]
  e <- dat$E[keep]

  y_resid <- resid(lm(y ~ e))
  x_resid <- resid(lm(x ~ e))

  tibble(
    x_resid = x_resid,
    y_resid = y_resid
  )
}

plot_partial_truth_vs <- function(mar_obj, x_method, label_method) {

  df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
    mutate(source = "Truth (full data)")

  dat_method <- mar_obj$dat_full
  dat_method[[x_method]] <- dplyr::coalesce(
    dat_method[[x_method]],
    dat_method[[x_method]]
  )
  df_method <- make_partial_df(dat_method, x_method) |>
    mutate(source = label_method)

  df_both <- bind_rows(df_truth, df_method)

  ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
      x = "Residualized nutrition (X | E)",
      y = "Residualized birthweight (Y | E)",
      color = NULL
    ) +
    theme_minimal(base_size = 14)
}
```

## Partial Regression Plot: Truth (Full Data)

- This slope is β_X under full data — the target line for every comparison.

```{r}
#| label: partial-truth

df_truth <- make_partial_df(mar_obj$dat_full, "X")

ggplot(df_truth, aes(x = x_resid, y = y_resid)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)"
  ) +
  theme_minimal(base_size = 14)
```

## Partial Plot: MAR Complete-Case, Naive (Y ~ X)

```{r}
#| label: partial-mar-cc-naive

cc <- mar_obj$cc

df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
  mutate(source = "Truth (full data)")

df_cc <- make_partial_df(cc |> mutate(X_cc = X_mar), "X_cc") |>
  mutate(source = "MAR CC: naive")

df_both <- bind_rows(df_truth, df_cc)

ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)",
    color = NULL
  ) +
  theme_minimal(base_size = 14)
```










## MAR: Listwise Deletion, Naive Model

```{r}
#| label: mar-naive
#| echo: true

mar_data <- simulate_birth_data(n = 1000, mcar_rate = 0.5, mar_logit_shift = 0.5)

mar_cc <- mar_data |> filter(!is.na(X_mar))

# Naive: ignore education
fit_mar_naive <- lm(Y ~ X_mar, data = mar_cc)
summary(fit_mar_naive)
```

## Full vs MCAR vs MAR: Means of X and E

```{r}
#| label: missingness-balance
#| echo: true

set.seed(606)
full_data <- simulate_birth_data(n = 1000, mcar_rate = 0.5, mar_logit_shift = 0.5)

# MCAR version of the same draw: random 50% missing on X
mcar_data <- full_data |>
  mutate(
    X_mcar_only = if_else(rbinom(n(), 1, 0.5) == 1, NA_real_, X)
  )

mar_cc   <- full_data |> filter(!is.na(X_mar))
mcar_cc  <- mcar_data |> filter(!is.na(X_mcar_only))

balance_tbl <- bind_rows(
  tibble(source = "Full data", mean_X = mean(full_data$X), mean_E = mean(full_data$E)),
  tibble(source = "MCAR (observed)", mean_X = mean(mcar_cc$X_mcar_only, na.rm = TRUE), mean_E = mean(mcar_cc$E)),
  tibble(source = "MAR (observed)",  mean_X = mean(mar_cc$X_mar), mean_E = mean(mar_cc$E))
)

balance_tbl
```

## MAR Scenarios: Missingness Level × Mechanism Strength

```{r}
#| label: mar-scenarios-table
#| echo: true

set.seed(999)
mar_scenarios <- generate_mar_scenarios(
  n = 1000,
  levels = c(0.1, 0.3, 0.5),
  shifts = c(0.5, 1, 1.5)
)

mar_summary <- summarize_mar_scenarios(mar_scenarios)
mar_summary
```

## MAR: Adjusting for Education

```{r}
#| label: mar-adjust
#| echo: true

fit_mar_adj <- lm(Y ~ X_mar + E, data = mar_cc)
summary(fit_mar_adj)
```


## MAR: Complete-Case Limitations

- Naive CC (Y ~ X only) is biased because we changed the education mix.
- If we condition on E, the slope for X improves — but we still threw away a lot of mothers and lost power.

```{r}
#| label: sim-impute-helpers
#| include: false

analyze_mar_strategies_once <- function(n = 1000, mcar_rate = 0.6) {
  dat <- simulate_birth_data(n = n, mcar_rate = mcar_rate)
  full_fit <- lm(Y ~ X + E, data = dat)

  beta_true <- coef(full_fit)[["X"]]

  cc <- dat |> filter(!is.na(X_mar))

  fit_cc_naive <- lm(Y ~ X_mar, data = cc)
  fit_cc_adj   <- lm(Y ~ X_mar + E, data = cc)

  mean_x <- mean(cc$X_mar, na.rm = TRUE)
  dat_mean <- dat |>
    mutate(
      X_imp_mean = if_else(is.na(X_mar), mean_x, X_mar)
    )
  fit_mean <- lm(Y ~ X_imp_mean + E, data = dat_mean)

  dat_mean_ind <- dat |>
    mutate(
      X_miss_ind = as.integer(is.na(X_mar)),
      X_imp_mean = if_else(is.na(X_mar), mean(cc$X_mar, na.rm = TRUE), X_mar)
    )
  fit_mean_ind <- lm(Y ~ X_imp_mean + X_miss_ind + E, data = dat_mean_ind)

  reg_x <- lm(X ~ E, data = dat, subset = !is.na(X_mar))
  dat_reg <- dat |>
    mutate(
      X_imp_reg = if_else(
        is.na(X_mar),
        predict(reg_x, newdata = dat),
        X_mar
      )
    )
  fit_reg <- lm(Y ~ X_imp_reg + E, data = dat_reg)

  summary_tbl <- tibble(
    method = c(
      "Full data (Y ~ X + E)",
      "MAR CC: naive (Y ~ X)",
      "MAR CC: adjust E",
      "Mean imputation",
      "Mean + missing indicator",
      "Regression imputation (E only)"
    ),
    estimate = c(
      coef(full_fit)[["X"]],
      coef(fit_cc_naive)[["X_mar"]],
      coef(fit_cc_adj)[["X_mar"]],
      coef(fit_mean)[["X_imp_mean"]],
      coef(fit_mean_ind)[["X_imp_mean"]],
      coef(fit_reg)[["X_imp_reg"]]
    ),
    se = c(
      coef(summary(full_fit))[["X", "Std. Error"]],
      coef(summary(fit_cc_naive))[["X_mar", "Std. Error"]],
      coef(summary(fit_cc_adj))[["X_mar", "Std. Error"]],
      coef(summary(fit_mean))[["X_imp_mean", "Std. Error"]],
      coef(summary(fit_mean_ind))[["X_imp_mean", "Std. Error"]],
      coef(summary(fit_reg))[["X_imp_reg", "Std. Error"]]
    ),
    beta_true = beta_true
  )

  list(
    dat_full   = dat,
    cc         = cc,
    dat_mean   = dat_mean,
    dat_mean_ind = dat_mean_ind,
    dat_reg    = dat_reg,
    full_fit   = full_fit,
    fit_cc_naive = fit_cc_naive,
    fit_cc_adj   = fit_cc_adj,
    fit_mean     = fit_mean,
    fit_mean_ind = fit_mean_ind,
    fit_reg      = fit_reg,
    summary      = summary_tbl
  )
}
```

```{r}
#| label: mar-strategies-setup
#| echo: false

mar_obj <- analyze_mar_strategies_once(n = 1000, mcar_rate = 0.6)
mar_strats <- mar_obj$summary
```

```{r}
#| label: partial-plot-helpers
#| include: false

make_partial_df <- function(dat, x_var) {
  # dat must contain Y, E, and the x_var column
  x <- dat[[x_var]]
  keep <- !is.na(x) & !is.na(dat$Y) & !is.na(dat$E)
  x <- x[keep]
  y <- dat$Y[keep]
  e <- dat$E[keep]

  y_resid <- resid(lm(y ~ e))
  x_resid <- resid(lm(x ~ e))

  tibble(
    x_resid = x_resid,
    y_resid = y_resid
  )
}

plot_partial_truth_vs <- function(mar_obj, x_method, label_method) {

  df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
    mutate(source = "Truth (full data)")

  dat_method <- mar_obj$dat_full
  dat_method[[x_method]] <- dplyr::coalesce(
    dat_method[[x_method]],
    dat_method[[x_method]]
  )
  df_method <- make_partial_df(dat_method, x_method) |>
    mutate(source = label_method)

  df_both <- bind_rows(df_truth, df_method)

  ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE) +
    labs(
      x = "Residualized nutrition (X | E)",
      y = "Residualized birthweight (Y | E)",
      color = NULL
    ) +
    theme_minimal(base_size = 14)
}
```

## Partial Regression Plot: Truth (Full Data)

- This slope is β_X under full data — the target line for every comparison.

```{r}
#| label: partial-truth

df_truth <- make_partial_df(mar_obj$dat_full, "X")

ggplot(df_truth, aes(x = x_resid, y = y_resid)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)"
  ) +
  theme_minimal(base_size = 14)
```

## Partial Plot: MAR Complete-Case, Naive (Y ~ X)

```{r}
#| label: partial-mar-cc-naive

cc <- mar_obj$cc

df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
  mutate(source = "Truth (full data)")

df_cc <- make_partial_df(cc |> mutate(X_cc = X_mar), "X_cc") |>
  mutate(source = "MAR CC: naive")

df_both <- bind_rows(df_truth, df_cc)

ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)",
    color = NULL
  ) +
  theme_minimal(base_size = 14)
```




## Naive CC (Y ~ X)

- Biased under MAR because we changed the education mix; the slope departs from the truth line.

## CC + E

- On the complete-case subset, conditioning on E aligns the slope with the truth line, but we still tossed many mothers and lost power.

## Partial Plot: Mean Imputation

```{r}
#| label: partial-mean

dat_mean <- mar_obj$dat_full |>
  mutate(
    X_imp_mean = if_else(is.na(X_mar), mean(X_mar, na.rm = TRUE), X_mar)
  )

df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
  mutate(source = "Truth (full data)")

df_mean <- make_partial_df(dat_mean, "X_imp_mean") |>
  mutate(source = "Mean imputation")

df_both <- bind_rows(df_truth, df_mean)

ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)",
    color = NULL
  ) +
  theme_minimal(base_size = 14)
```

## Mean Imputation

- Shrinks the slope toward zero relative to the truth line and understates SEs — overconfident and biased.

## Partial Plot: Mean Imputation + Missing Indicator

```{r}
#| label: partial-mean-ind

dat_mean_ind <- mar_obj$dat_full |>
  mutate(
    X_imp_mean = if_else(is.na(X_mar), mean(X_mar, na.rm = TRUE), X_mar)
  )

df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
  mutate(source = "Truth (full data)")

df_mean_ind <- make_partial_df(dat_mean_ind, "X_imp_mean") |>
  mutate(source = "Mean + indicator")

df_both <- bind_rows(df_truth, df_mean_ind)

ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)",
    color = NULL
  ) +
  theme_minimal(base_size = 14)
```

## Mean + Indicator

- Geometry mirrors mean imputation; the indicator lives in the model, but the slope still drifts from the truth line and variance behavior is unpredictable.

## Partial Plot: Regression Imputation Using E

```{r}
#| label: partial-reg

dat_reg <- mar_obj$dat_full |>
  mutate(
    X_imp_reg = if_else(
      is.na(X_mar),
      predict(lm(X ~ E, data = mar_obj$dat_full, subset = !is.na(X_mar)),
              newdata = mar_obj$dat_full),
      X_mar
    )
  )

df_truth <- make_partial_df(mar_obj$dat_full, "X") |>
  mutate(source = "Truth (full data)")

df_reg <- make_partial_df(dat_reg, "X_imp_reg") |>
  mutate(source = "Regression imputation (E)")

df_both <- bind_rows(df_truth, df_reg)

ggplot(df_both, aes(x = x_resid, y = y_resid, color = source)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "Residualized nutrition (X | E)",
    y = "Residualized birthweight (Y | E)",
    color = NULL
  ) +
  theme_minimal(base_size = 14)
```

## Regression Imputation (E)

- Slope often lands near the truth line, but uncertainty is still too small because predicted X values are treated as known.

## MAR Strategies Table

```{r}
#| label: mar-strategies-table
#| echo: true

mar_strats
```

## Summary: Comparing Strategies Under MAR

```{r}
#| label: mar-strategies-plot
#| fig-cap: "Inferior fixes: some recover the slope but all mishandle uncertainty in different ways."

ggplot(mar_strats, aes(x = method, y = estimate)) +
  geom_hline(aes(yintercept = beta_true), linetype = 2, color = "gray40") +
  geom_pointrange(aes(ymin = estimate - 1.96 * se,
                      ymax = estimate + 1.96 * se)) +
  coord_flip() +
  labs(
    x = NULL,
    y = "Estimated slope of nutrition → birthweight"
  ) +
  theme_minimal(base_size = 14)
```

- Dashed line is the true β_X from the full-data model.
- Each point/interval shows what that method does to β_X on this same simulated scenario.
- Earlier partial plots were microscope views of the same story.

## MNAR: No Exit Without Extra Information

- Missingness depends on the unobserved value itself (e.g., lowest birthweights least likely to be recorded).
- No amount of modeling with observed covariates can identify the truth without extra assumptions or data.
- We handle MNAR with sensitivity analyses, external validation data, or explicit missingness models — not today's focus.

## Takeaways

- MCAR: loss of precision, estimates remain centered on truth.
- MAR: can be handled if we measure and model drivers of missingness (e.g., education), but naive complete-case can bias results.

## Takeaways (cont.)

- Simple single-imputation tricks (mean, mean+indicator, regression-only) often distort the slope or understate uncertainty.
- MNAR: requires assumptions or extra data; can't be fixed by clever modeling of observed variables alone.

## Next Up (not yet shown)

- Proper MI workflow: impute → analyze → pool (Rubin's rules).
- Diagnostics: compare distributions, fraction of missing information.
- Show how MI behaves on the same birthweight / education example, using the same code scaffolding.
